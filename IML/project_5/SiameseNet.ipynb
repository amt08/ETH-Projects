{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "my_seed = 1508\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(my_seed)\n",
    "import random\n",
    "random.seed(my_seed)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(my_seed)\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.layers import Dense, Flatten, Input, Dropout, Activation, Conv2D, MaxPool2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.initializers import he_uniform, he_normal\n",
    "from keras.applications.resnet50 import preprocess_input as preprocess_input_resnet\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Triplet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse triplet file and return a list of lists of three image indices\n",
    "def process_triplets(tuple_file_path):\n",
    "    triplet_tuples = []\n",
    "    with open(tuple_file_path) as fp:\n",
    "        for line in fp:\n",
    "            triplet_tuples.append([int(index.replace('\\n','')) for index in line.split(' ')])\n",
    "    triplet_tuples = np.array(triplet_tuples)\n",
    "    return triplet_tuples\n",
    "\n",
    "\n",
    "def train_val_split_images(triplets):\n",
    "    unique_images = np.unique(triplets)\n",
    "    train_image_ids = unique_images\n",
    "    np.random.seed(11)\n",
    "    permute_indices = np.random.permutation(len(train_image_ids))\n",
    "    train_image_ids_perm = train_image_ids[permute_indices]\n",
    "    perc_train = round(len(train_image_ids_perm) * 0.9)\n",
    "    train_images = train_image_ids_perm[0: perc_train]\n",
    "    val_images = train_image_ids_perm[perc_train:]\n",
    "    \n",
    "    return train_images, val_images\n",
    "\n",
    "\n",
    "def train_val_split_triplets(triplets, train_images, val_images):\n",
    "    train_triplet_ids = list()\n",
    "    val_triplets_ids = list()\n",
    "    for i, triplet in enumerate(triplets):\n",
    "        if np.array([img in train_images for img in triplet]).all():\n",
    "            train_triplet_ids.append(i)\n",
    "        elif np.array([img in val_images for img in triplet]).all():\n",
    "            val_triplets_ids.append(i)\n",
    "            \n",
    "    train_triplets = triplets[train_triplet_ids]\n",
    "    val_triplets = triplets[val_triplets_ids]\n",
    "    \n",
    "    return train_triplets, val_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets_full = process_triplets('train_triplets.txt')\n",
    "print(train_triplets_full.shape)\n",
    "\n",
    "output_triplets = process_triplets('test_triplets.txt')\n",
    "print(output_triplets.shape)\n",
    "\n",
    "train_idx, text_idx = train_val_split_images(train_triplets_full)\n",
    "train_triplets, test_triplets = train_val_split_triplets(train_triplets_full, train_idx, text_idx)\n",
    "\n",
    "print(train_triplets.shape, test_triplets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_path = 'food'\n",
    "img_H = 209\n",
    "img_W = 300\n",
    "img_C = 3\n",
    "\n",
    "# read all images in memory, resize, preprocess, normalize\n",
    "def food_proc(flip=False):\n",
    "    food = np.zeros((len(os.listdir(food_path)), img_H, img_W, img_C))\n",
    "    for index, img in tqdm(enumerate(os.listdir(food_path))):\n",
    "\n",
    "        # load an image from file\n",
    "        image = load_img(os.path.join(food_path, img), target_size=(img_H, img_W))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # flip image left ro right (horizontally)\n",
    "        if flip:\n",
    "            tf.random.set_seed(my_seed)\n",
    "            image = tf.image.flip_left_right(image)\n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input_resnet(image)\n",
    "\n",
    "        food[index] = image\n",
    "\n",
    "    print(food.shape)\n",
    "    \n",
    "    return food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process food images\n",
    "food = food_proc(flip=False)\n",
    "#with open('resnet_food_'+str(img_H)+'_'+str(img_W)+'.pkl','wb') as f: pkl.dump(food, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process flipped food images\n",
    "#food_flip = food_proc(flip=True)\n",
    "#with open('resnet_food_flip_'+str(img_H)+'_'+str(img_W)+'.pkl','wb') as f: pkl.dump(food_flip, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load and test food images\n",
    "#with open('resnet_food_'+str(img_H)+'_'+str(img_W)+'.pkl','rb') as f: food = pkl.load(f)\n",
    "#print(food.shape)\n",
    "\n",
    "#plt.figure(figsize=(6, 2))\n",
    "#plt.imshow((food[9999]).astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and test flipped food images\n",
    "#with open('resnet_food_flip_'+str(img_H)+'_'+str(img_W)+'.pkl','rb') as f: food_flip = pkl.load(f)\n",
    "#print(food_flip.shape)\n",
    "\n",
    "#plt.figure(figsize=(6, 2))\n",
    "#plt.imshow((food_flip[9999]*255.).astype(np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use pretrained vgg model with the output layer replaced by a max-pooling layer\n",
    "mdl = ResNet50(include_top=False, input_shape=(img_H, img_W, img_C))\n",
    "\n",
    "pretrained_model = Model(inputs=mdl.inputs, outputs=mdl.layers[-1].output)\n",
    "\n",
    "m_shape = pretrained_model.layers[-1].output_shape\n",
    "\n",
    "# pretrained_model.summary()\n",
    "print(m_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings from the pretrained model for the food images\n",
    "\n",
    "def embed_food(food):\n",
    "    print(food.shape)\n",
    "    food_emb = np.zeros((food.shape[0], m_shape[1], m_shape[2], m_shape[3]))\n",
    "    for i in tqdm(range(0, food.shape[0])):\n",
    "        food_emb[i:i+1] = pretrained_model.predict(food[i:i+1])\n",
    "    return food_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed food images\n",
    "food_emb = embed_food(food)\n",
    "#with open('resnet_food_emb_'+str(img_H)+'_'+str(img_W)+'.pkl','wb') as f: pkl.dump(food_emb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed flip food images\n",
    "#food_flip_emb = embed_food(food_flip)\n",
    "#with open('resnet_food_flip_emb_'+str(img_H)+'_'+str(img_W)+'.pkl','wb') as f: pkl.dump(food_flip_emb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and test food embeddings\n",
    "#with open('resnet_food_emb_'+str(img_H)+'_'+str(img_W)+'.pkl','rb') as f: food_emb = pkl.load(f)\n",
    "#print(food_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and test flipped food embeddings\n",
    "#with open('resnet_food_flip_emb_'+str(img_H)+'_'+str(img_W)+'.pkl','rb') as f: food_flip_emb = pkl.load(f)\n",
    "#print(food_flip_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a triplet of images\n",
    "def get_triplet_images(triplet_indices):\n",
    "    return [food_emb[triplet_indices[0]], food_emb[triplet_indices[1]], food_emb[triplet_indices[2]]]\n",
    "\n",
    "def get_triplet_images_flip(triplet_indices):\n",
    "    # TODO: fix the below since this function is actually used later on\n",
    "    return [food_flip_emb[triplet_indices[0]], food_flip_emb[triplet_indices[1]], food_flip_emb[triplet_indices[2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = get_triplet_images(train_triplets[0])\n",
    "sample[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(batch_size, triplet_list):\n",
    "    # corresponding zero arrays\n",
    "    x_anchors = np.zeros((batch_size, m_shape[1], m_shape[2], m_shape[3]))\n",
    "    x_positives = np.zeros((batch_size, m_shape[1], m_shape[2], m_shape[3]))\n",
    "    x_negatives = np.zeros((batch_size, m_shape[1], m_shape[2], m_shape[3]))\n",
    "    \n",
    "    random.seed(my_seed)\n",
    "    rand_list = random.sample(range(0, len(triplet_list)), batch_size)\n",
    "    \n",
    "    for i, random_index in enumerate(rand_list):\n",
    "\n",
    "        triplet = get_triplet_images(triplet_list[random_index])\n",
    "        \n",
    "        x_anchors[i] = triplet[0]\n",
    "        x_positives[i] = triplet[1]\n",
    "        x_negatives[i] = triplet[2]\n",
    "    \n",
    "    return [x_anchors, x_positives, x_negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = create_batch(1, train_triplets)\n",
    "sample[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, triplet_list, batch_size, shuffle=False, flip=False):\n",
    "        self.triplet_list = triplet_list\n",
    "        self.indices = np.arange(0, self.triplet_list.shape[0])\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.flip = flip\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.triplet_list.shape[0] % self.batch_size == 0:\n",
    "            return int(self.triplet_list.shape[0]/self.batch_size)\n",
    "        else:\n",
    "            return int(self.triplet_list.shape[0]/self.batch_size+1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "\n",
    "        if self.flip == False:\n",
    "            X, y = self.__get_data(batch)\n",
    "        else:\n",
    "            X, y = self.__get_flip_data(batch)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(0, self.triplet_list.shape[0])\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __get_data(self, btc):\n",
    "        # corresponding zero arrays\n",
    "        x_anchors = np.zeros((len(btc), m_shape[1], m_shape[2], m_shape[3]))\n",
    "        x_positives = np.zeros((len(btc), m_shape[1], m_shape[2], m_shape[3]))\n",
    "        x_negatives = np.zeros((len(btc), m_shape[1], m_shape[2], m_shape[3]))\n",
    "\n",
    "        for i in range(0, len(btc)):\n",
    "            \n",
    "            triplet = get_triplet_images(self.triplet_list[btc[i]])\n",
    "\n",
    "            x_anchors[i] = triplet[0]\n",
    "            x_positives[i] = triplet[1]\n",
    "            x_negatives[i] = triplet[2]\n",
    "            \n",
    "        X = [x_anchors, x_positives, x_negatives]\n",
    "        y = np.zeros((len(btc), 3*emb_size))\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "        \n",
    "    def __get_flip_data(self, btc):\n",
    "        # corresponding zero arrays\n",
    "        x_anchors = np.zeros((len(btc)*2, m_shape[1], m_shape[2], m_shape[3]))\n",
    "        x_positives = np.zeros((len(btc)*2, m_shape[1], m_shape[2], m_shape[3]))\n",
    "        x_negatives = np.zeros((len(btc)*2, m_shape[1], m_shape[2], m_shape[3]))\n",
    "\n",
    "        for idx, i in enumerate(range(0, len(btc)*2, 2)):\n",
    "            \n",
    "            triplet = get_triplet_images(self.triplet_list[btc[idx]])\n",
    "\n",
    "            x_anchors[i] = triplet[0]\n",
    "            x_positives[i] = triplet[1]\n",
    "            x_negatives[i] = triplet[2]\n",
    "            \n",
    "        for idx, i in enumerate(range(1, len(btc)*2, 2)):\n",
    "            \n",
    "            triplet = get_triplet_images_flip(self.triplet_list[btc[idx]])\n",
    "\n",
    "            x_anchors[i] = triplet[0]\n",
    "            x_positives[i] = triplet[1]\n",
    "            x_negatives[i] = triplet[2]\n",
    "            \n",
    "        X = [x_anchors, x_positives, x_negatives]\n",
    "        y = np.zeros((len(btc), 3*emb_size))\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainable Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 64 \n",
    "drop_val = 0.8\n",
    "\n",
    "def gen_emb_model(emb_seed=my_seed):\n",
    "    embedding_model = Sequential()\n",
    "\n",
    "    embedding_model.add(Input(shape=(m_shape[1], m_shape[2], m_shape[3],)))\n",
    "\n",
    "    embedding_model.add(BatchNormalization())\n",
    "    embedding_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    embedding_model.add(Dropout(drop_val, seed=emb_seed))\n",
    "\n",
    "    embedding_model.add(Flatten())\n",
    "\n",
    "    embedding_model.add(Dense(emb_size, activation='sigmoid', kernel_initializer = he_normal(seed=emb_seed)))\n",
    "    \n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try embedding model\n",
    "embedding_model = gen_emb_model()\n",
    "embedding_model.summary()\n",
    "\n",
    "example = np.expand_dims(get_triplet_images(train_triplets[0])[0], axis=0)\n",
    "example_emb = embedding_model.predict(example)[0]\n",
    "\n",
    "print(example_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imput layers for each of the three images\n",
    "\n",
    "def get_siam_net(emb_seed=my_seed):\n",
    "\n",
    "    embedding_model = gen_emb_model(emb_seed=emb_seed)\n",
    "    \n",
    "    input_anchor = Input(shape=(m_shape[1], m_shape[2], m_shape[3],))\n",
    "    input_positive = Input(shape=(m_shape[1], m_shape[2], m_shape[3],))\n",
    "    input_negative = Input(shape=(m_shape[1], m_shape[2], m_shape[3],))\n",
    "\n",
    "    # get embeddings from the embedding model defined above\n",
    "    embedding_anchor = embedding_model(input_anchor)\n",
    "    embedding_positive = embedding_model(input_positive)\n",
    "    embedding_negative = embedding_model(input_negative)\n",
    "\n",
    "    # concatenate the embeddings ready for the triplet loss function\n",
    "    output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=1)\n",
    "\n",
    "    net = Model([input_anchor, input_positive, input_negative], output)\n",
    "    \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.35\n",
    "\n",
    "# cusom triplet loss function\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    # slice the concatenated embeddings\n",
    "    anchor, positive, negative = y_pred[:,:emb_size], y_pred[:,emb_size:2*emb_size], y_pred[:,2*emb_size:]\n",
    "    # calculate the distances A-B, A-C\n",
    "    positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=1)\n",
    "    negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=1)\n",
    "    \n",
    "    # apply max and alpha\n",
    "    return tf.maximum(positive_dist - negative_dist + alpha, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_output(output, model):\n",
    "    if model != None:\n",
    "        emb_size = int(model.layers[-1].output.shape[1] / 3)\n",
    "    anchor, positive, negative = output[:,:emb_size], output[:,emb_size:2*emb_size], output[:,2*emb_size:]\n",
    "    positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=1)\n",
    "    negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=1)\n",
    "    return (1 if positive_dist < (negative_dist) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy_batch(net, triplets, items, model=None, tr_set='', quiet=False):\n",
    "    count = 0\n",
    "    \n",
    "    outputs = net.predict(create_batch(items, triplets))\n",
    "    \n",
    "    for index in range(0, outputs.shape[0]):\n",
    "        if eval_output(outputs[index:index+1], model) == 1:\n",
    "            count += 1\n",
    "\n",
    "    res = count/items * 100\n",
    "    \n",
    "    if quiet==False:\n",
    "        print(tr_set+':',res)\n",
    "    \n",
    "    return res\n",
    "    \n",
    "\n",
    "def eval_accuracy(net, triplets, model=None, tr_set='', quiet=False):\n",
    "    count = 0\n",
    "    \n",
    "    x_A = np.zeros((len(triplets), m_shape[1], m_shape[2], m_shape[3]))\n",
    "    x_B = np.zeros((len(triplets), m_shape[1], m_shape[2], m_shape[3]))\n",
    "    x_C = np.zeros((len(triplets), m_shape[1], m_shape[2], m_shape[3]))\n",
    "    \n",
    "    for index in range(0, len(triplets)):\n",
    "        \n",
    "        triplet = get_triplet_images(triplets[index])\n",
    "        \n",
    "        x_A[index] = triplet[0]\n",
    "        x_B[index] = triplet[1]\n",
    "        x_C[index] = triplet[2]\n",
    "        \n",
    "    outputs = net.predict([x_A, x_B, x_C])\n",
    "    \n",
    "    \n",
    "    for index in range(0, outputs.shape[0]):\n",
    "        if eval_output(outputs[index:index+1], model) == 1:\n",
    "            count += 1\n",
    "            \n",
    "    res = count/len(triplets) * 100\n",
    "    \n",
    "    if quiet==False:\n",
    "        print(tr_set+':', res)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "net = get_siam_net(emb_seed=123)\n",
    "\n",
    "opt = RMSprop(learning_rate=0.001)\n",
    "net.compile(loss=triplet_loss, optimizer=opt)\n",
    "\n",
    "_ = net.fit(\n",
    "    DataGenerator(train_triplets, batch_size=batch_size, shuffle=False, flip=False),\n",
    "    epochs=epochs, verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "eval_accuracy_batch(net, train_triplets, 100, model=net, tr_set=' Train Acc')\n",
    "eval_accuracy(net, test_triplets, model=net, tr_set='  Test Acc')\n",
    "eval_accuracy_batch(net, output_triplets, 100, model=net, tr_set='Output Acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_output(net, triplets, out_document, batch_size=128):\n",
    "    \n",
    "    dg = DataGenerator(triplets, batch_size=batch_size, shuffle=False, flip=False)\n",
    "    \n",
    "    output_list = []\n",
    "    preds = []\n",
    "    \n",
    "    for index in tqdm(range(0, dg.__len__())):\n",
    "        in_list = dg.__getitem__(index)\n",
    "        pred = net.predict(in_list)\n",
    "        \n",
    "        for p in pred:\n",
    "            preds.append(p)\n",
    "        \n",
    "    \n",
    "    with open(out_document, 'w') as out_file:\n",
    "        for index in tqdm(range(0, len(preds))):\n",
    "            eval_pred = eval_output(preds[index].reshape((1, -1)), net)\n",
    "            output_list.append(eval_pred)\n",
    "            out_file.write(str(eval_pred)+'\\n')\n",
    "        \n",
    "    return output_list, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list, preds = gen_output(net, output_triplets, 'results.txt', batch_size=64)\n",
    "\n",
    "print(len(output_list))\n",
    "\n",
    "count = 0\n",
    "for i in output_list:\n",
    "    if i == 1:\n",
    "        count+=1\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "39b5c16b68b67ba22346ef6d698bf62f4f1e91a475e577be2818f6d4b05b72b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
