{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW05: Deep Learning\n",
    "\n",
    "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO create a new variable \"business\" that takes value 1 if the label is business and 0 otherwise\n",
    "df['business'] = df['label'].apply(lambda x: 1 if x == 'business' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "##TODO pre-process text as you did in HW02\n",
    "def preprocess(text):\n",
    "    return [wd.lemma_.lower() for wd in list(nlp(text)) if not wd.is_punct and not wd.is_stop]\n",
    "\n",
    "df['tokens'] = df['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c95bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_clean'] = df['tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO vectorize the pre-processed text using CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(min_df=0.01,\n",
    "                        max_df=.9,  \n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        ngram_range=(1,2))\n",
    "X = vec.fit_transform(df['tokens_clean'])\n",
    "\n",
    "features = vec.get_feature_names()\n",
    "Y = df['business']\n",
    "\n",
    "#Alternatively, use the output from HW02 if you saved it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal here is to use features from the Vectorized text to predict whether the snippet is from a business article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "## TODO build a MLP model with at least 2 hidden layers with ReLU activation,\n",
    "# followed by dropout and an output layer with sigmoid activation\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim = X.shape[1], activation = 'relu'))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "## TODO compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## TODO fit the model using early stopping to predict the business label\n",
    "es = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,\n",
    "                   patience=3, mode='auto')\n",
    "\n",
    "fit = model.fit(X.todense(), Y, batch_size=64, epochs=100, callbacks=[es], validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "##TODO build a simple autoencoder with two compression layers and two reconstruction layers using ReLu\n",
    "model_a = Sequential()\n",
    "model_a.add(Dense(100, input_dim = X.shape[1], activation = 'relu'))\n",
    "model_a.add(Dense(50, activation = 'relu', name = 'compression_2'))\n",
    "model_a.add(Dense(100, activation = 'relu')) # first reconstruction layer\n",
    "model_a.add(Dense(X.shape[1], activation = 'relu'))\n",
    "model_a.summary()\n",
    "\n",
    "##TODO compile and fit the model minimizing \"mean_squared_error\"\n",
    "##report r_squared during training (the function r2 defined above)\n",
    "model_a.compile(loss='mean_squared_error', optimizer = 'adam', metrics=[r2])\n",
    "model_a_summary = model_a.fit(X.todense(), X.todense(), epochs=10, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "##TODO compress the vectorized text (X.todense())\n",
    "compression = keras.Model(inputs = model_a.input, outputs = model_a.get_layer(\"compression_2\").output)\n",
    "X_compressed = compression(X.todense())\n",
    "print(f'Compressed size: {X_compressed.shape}')\n",
    "print(f'Original size: {X.todense().shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "##TODO tokenize the text using text_to_word_sequence\n",
    "tokenized_text = [text_to_word_sequence(text) for text in df['text']]\n",
    "\n",
    "print(tokenized_text[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "length_vocab = 1000\n",
    "max_seq_length = 100\n",
    "\n",
    "#TODO create a one_hot representation for each word and truncate/pad the sequences such that they are all of the same length\n",
    "X_one_hot = [one_hot(token, n=length_vocab) for token in df['text']]\n",
    "\n",
    "X_padded = pad_sequences(X_one_hot, padding='post', \n",
    "                         maxlen=max_seq_length, truncating='post')\n",
    "X_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Embedding\n",
    "\n",
    "##TODO create a sequential model with just one embedding layer and show the model summary\n",
    "model_em = Sequential()\n",
    "model_em.add(Embedding(length_vocab,32,input_length=max_seq_length))\n",
    "model_em.summary() with just one embedding layer and show the model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "##TODO create a sequential model with an embedding layer,\n",
    "# a LSTM layer and two hidden layers with ReLu activation function, followed by dropout\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(length_vocab,32,input_length=max_seq_length,name='embedding'))\n",
    "model_lstm.add(LSTM(32))\n",
    "model_lstm.add(Dense(32, activation='relu'))\n",
    "model_lstm.add(Dense(32, activation='relu'))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(1, activation=\"sigmoid\"))\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO compile the model and fit it to predict the business label\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_lstm.fit(X_padded, Y, batch_size=32, validation_split=0.3, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
