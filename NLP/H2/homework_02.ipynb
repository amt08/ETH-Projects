{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW02: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "dfs = df.sample(50)\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d374df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO use spacy to split the documents in the sampled dataframe (dfs) in sentences and tokens\n",
    "dfs['sentences'] = dfs['text'].apply(lambda x: list(nlp(x).sents))\n",
    "dfs['tokens'] = dfs['text'].apply(lambda x: list(nlp(x)))\n",
    "\n",
    "##TODO print the first sentence of the first document in your sample\n",
    "dfs.iloc[0, 4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO create a new column with tokens in lowercase (x.lower()), without punctuation tokens (x.is_punct) nor stopwords (x.is_stop)\n",
    "\n",
    "def preprocess(text):\n",
    "    return [wd.lemma_.lower() for wd in list(nlp(text)) if not wd.is_punct and not wd.is_stop]\n",
    "\n",
    "dfs['tokens_lower'] = dfs['text'].apply(lambda x: preprocess(x))\n",
    "\n",
    "##TODO print the tokens (x.lemma_) and the tags (x.tag_ ) of the first sentence of the first document (doc.sents)\n",
    "sentence_1 = dfs.iloc[0, 4][0]\n",
    "\n",
    "print('****** x.lemma_******')\n",
    "print([x.lemma_ for x in sentence_1])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('****** x.tag_******')\n",
    "print([x.tag_ for x in sentence_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print the first 20 noun chuncks in your sample corpus (doc.noun_chunks)\n",
    "i = 0\n",
    "for nc in nlp('. '.join(dfs['text'].values[0:50])).noun_chunks:\n",
    "    print(f'{nc} - {nc.label_}')\n",
    "    \n",
    "    if i >= 19:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities\n",
    "\n",
    "Let's compute the ratio of named entities starting with a capital letter, e.g. if we have \"University of Chicago\" as a NE, \"University\" and \"Chicago\" are capitalized, \"of\" is not, thus the ratio is 2/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print the ratio of tokens being part of a named entity span starting with a capital letter (doc.ents)\n",
    "corpus = '. '.join(dfs['text'].values)\n",
    "\n",
    "# all capitalized tokens in the NE span / all tokens in the NE span\n",
    "sum([len([t for t in ent if t.text[0].isupper()]) for ent in nlp(corpus).ents]) / \\\n",
    "sum([len([t for t in ent]) for ent in nlp(corpus).ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all capitalized tokens in the NE span / all tokens at a row level\n",
    "\n",
    "ratio = [upper / total for upper, total in zip([len([t for t in ent if t.text[0].isupper()]) \\\n",
    "     for ent in nlp(corpus).ents], [len([t for t in ent]) \\\n",
    "                                          for ent in nlp(corpus).ents])]\n",
    "\n",
    "entity = [[t for t in ent] for ent in nlp(corpus).ents]\n",
    "\n",
    "pd.DataFrame(list(zip(entity, ratio))).iloc[55:60] # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print the ratio of capitalized tokens not being part of a named entity span\n",
    "# e.g. \"The dog barks\" = 1/3; 3 tokens, only \"The\" is capitalized\n",
    "\n",
    "entities = [list(nlp(ent.text)) for ent in nlp(corpus).ents]\n",
    "\n",
    "# flatten the entities list into one list\n",
    "flat_list = [item.text for sublist in entities for item in sublist]\n",
    "\n",
    "# all tokens in corpus\n",
    "all_tokens = list(nlp(corpus))\n",
    "\n",
    "# all capitalized tokens not part of the NE span / all tokens not in the NE span\n",
    "len([word for word in all_tokens if \\\n",
    "     word.text not in flat_list and word.text[0].isupper()]) / len([word for word in all_tokens if \\\n",
    "                                                                    word.text not in flat_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print the ratio of capitalized tokens not being a named entity and not being the first token in a sentence\n",
    "# e.g. \"The dog barks\" = 0; 3 tokens, \"The\" is capitalized but the starting token of a sentence,\n",
    "# no other tokens are capitalized.\n",
    "\n",
    "first_in_sentence = [sent[0].text for sent in nlp(corpus).sents] # all starting words in sentences\n",
    "\n",
    "# all capitalized tokens not part of the NE span and not a first word in a sentence / all tokens not in the NE span\n",
    "len([word for word in all_tokens if \\\n",
    "     word.text not in flat_list and \\\n",
    "     word.text not in first_in_sentence and \\\n",
    "     word.text[0].isupper()]) / \\\n",
    "len([word for word in all_tokens if word.text not in flat_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word for word in all_tokens \\\n",
    " if word.text not in flat_list and \n",
    " word.text not in first_in_sentence and \n",
    " word.text[0].isupper()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give an example of a capitalized token in the data which is neither a named entity nor at the start of a sentence. What could be the reason the token is capitalized (one sentence)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0bb03",
   "metadata": {},
   "source": [
    "Maybe just the fact that some words are part of a title of the news that has only capital letters but it is clearly not recognised as a named entitiy, e.g. **Elephants** which is part of the title \"South Africa Considers Killing Elephants\", as part of the text variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=0.01, \n",
    "                        max_df=0.9,  \n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        use_idf=True, # the new piece\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "##TODO using the whole sample, produce a world cloud with bigrams for each label using tfidf frequencies\n",
    "X_tfidf = tfidf.fit_transform(df['text'])\n",
    "vocab = tfidf.get_feature_names()\n",
    "for label in label_map.values(): \n",
    "    slicer = df['label'] == label\n",
    "    f = X_tfidf[slicer.values]\n",
    "    total_freqs = list(np.array(f.sum(axis=0))[0])\n",
    "    fdict = dict(zip(vocab,total_freqs))\n",
    "    bigram_dict = dict()\n",
    "    for key, value in fdict.items():\n",
    "        if len(key.split()) > 1:\n",
    "            bigram_dict[key] = value\n",
    "    wordcloud = WordCloud().generate_from_frequencies(bigram_dict) \n",
    "    print(label)\n",
    "    plt.clf()\n",
    "    plt.imshow(wordcloud, interpolation='bilinear') \n",
    "    plt.axis(\"off\") \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from eli5.sklearn import InvertableHashingVectorizer\n",
    "\n",
    "hv = HashingVectorizer(n_features=5000)\n",
    "\n",
    "##TODO print the first 10 features produced by the hash vectorizer\n",
    "X_hash = hv.fit_transform(df['text'])\n",
    "\n",
    "inv_vectorizer = InvertableHashingVectorizer(hv)\n",
    "inverted_hv = inv_vectorizer.fit(df['text'])\n",
    "# first 10 features\n",
    "[i for i in inverted_hv.get_feature_names()][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d460f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(min_df=0.01,max_df=.9, max_features=1000, stop_words='english', ngram_range=(1,2))\n",
    "X = vec.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "\n",
    "##TODO compute the number of words per document (excluding stopwords)\n",
    "def tokenize(text):\n",
    "    return [wd.lemma_.lower() for wd in list(nlp(text)) if not wd.is_stop]\n",
    "\n",
    "df['no_words'] = df['text'].apply(lambda x: len(tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c5474",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO get the most predictive features of the number of words per document using first f_class and then chi2\n",
    "\n",
    "# using f_classif\n",
    "select_features = SelectKBest(f_classif, k = 10)\n",
    "Y = df['no_words']\n",
    "select_features.fit(X, Y)\n",
    "[vocab[i] for i in np.argsort(select_features.scores_)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using chi2\n",
    "select_chi = SelectKBest(chi2, k = 10)\n",
    "result = select_chi.fit_transform(X, Y)\n",
    "[vocab[i] for i in np.argsort(select_chi.scores_)[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results different? What could be a reason for this? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30295806",
   "metadata": {},
   "source": [
    "Yes, the results seem to be different. The chi-square test measures the dependence between variables, so using this function removes the features that are the most likely to be independent of the predicted variable. Also chi2 is used in relation to a categorical output but in our case we have a continous one, so maybe that has an impact on the 'quality' of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we use distilbert tokenizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# let's instantiate a tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "##TODO tokenize the sentences in the sampled dataframe (dfs) using the DisilBertTokenizer\n",
    "dfs['bert_tokens'] = dfs['sentences'].apply(lambda x: tokenizer.tokenize(str(x)))\n",
    "\n",
    "##TODO what is the type/token ratio from this tokenizer (number_of_unqiue_token_types/number_of_tokens)?\n",
    "dfs['unique_token_ratio'] = dfs['bert_tokens'].apply(lambda x: len(list(set(x)))/len(x))\n",
    "\n",
    "##TODO what is the amount of subword tokens returned by the huggingface tokenizer? hint: each subword token starts with \"#\"\n",
    "dfs['subwords_count'] = dfs['bert_tokens'].apply(lambda x: len([item for item in x if item.startswith('#')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c46f5ccc24ef453e97fd721763d326d36913f44a8394821f5f9ff49f0aa076c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
