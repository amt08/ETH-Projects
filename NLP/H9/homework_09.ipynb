{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "present-brown",
   "metadata": {},
   "source": [
    "# HW09: Transformers\n",
    "\n",
    "Remember that these homework work as a completion grade. **You can skip one section of this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-klein",
   "metadata": {},
   "source": [
    "## Hugginface Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig, DistilBertTokenizerFast\n",
    "import tensorflow as tf\n",
    "\n",
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased')\n",
    "config.num_labels = 4\n",
    "transformer_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_id_mask(data, length):\n",
    "    data_tf = [tokenizer(text, return_tensors='tf',\n",
    "              max_length=length) for text in data]\n",
    "    input_ids, input_masks = [x[\"input_ids\"][0].numpy() for x in data_tf], [x[\"attention_mask\"][0].numpy() for x in data_tf]\n",
    "    \n",
    "    return input_ids, input_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6116326",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO split the sample into a training and a test set \n",
    "##TODO prepare the dataset for tensorflow.\n",
    "\n",
    "# labels need to be numbers actually\n",
    "from sklearn.model_selection import train_test_split\n",
    "df['target'] = pd.factorize(df['label'])[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'].tolist(),\n",
    "                                                    df['target'].tolist(),\n",
    "                                                    test_size=0.3)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "input_ids, input_masks = out_id_mask(X_train, 32)\n",
    "input_ids_test, input_masks_test = out_id_mask(X_test, 32)\n",
    "\n",
    "index = [i for i, elem in enumerate(input_ids) if len(elem) < 32]\n",
    "index_test = [i for i, elem in enumerate(input_ids_test) if len(elem) < 32]\n",
    "\n",
    "for item in sorted(index, reverse=True):\n",
    "    del input_ids[item]\n",
    "    del input_masks[item]\n",
    "    del y_train[item]\n",
    "    \n",
    "for item in sorted(index_test, reverse=True):\n",
    "    del input_ids_test[item]\n",
    "    del input_masks_test[item]\n",
    "    del y_test[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO build a transformer model to do sequence classification with the goal to predict the label from the text\n",
    "in_ids = tf.keras.layers.Input(shape=(32,), name='input_token', dtype='int32')\n",
    "in_masks_ids = tf.keras.layers.Input(shape=(32,), name='masked_token', dtype='int32')\n",
    "X = transformer_model(in_ids, in_masks_ids)\n",
    "model = tf.keras.Model(inputs=[in_ids, in_masks_ids], outputs = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO compile the model\n",
    "# losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(({'input_token': input_ids,\n",
    "                                               'masked_token': input_masks}, y_train)).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO fit the model and print the obtained accuracy\n",
    "model.fit(dataset, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-packet",
   "metadata": {},
   "source": [
    "**Hint:** All the vectorized pieces of text must have the same length (which will be equal to the input size). You have two options to ensure this:\n",
    "\n",
    "1. Set the maximum length equal to the length of the shortest vectorized text\n",
    "2. Choose the maximum length and then exclude all the data points that have vectors shorter than that length\n",
    "\n",
    "**Hint:** Tensorflow requires your labels to be integers, not strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6408718",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices(({'input_token': input_ids_test,\n",
    "                                               'masked_token': input_masks_test})).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predictions = tf.nn.softmax(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7809a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [np.argmax(item) for item in predictions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-invention",
   "metadata": {},
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "##TODO Pick one snippet for each label and generate some starting from the first 4-5 words\n",
    "## pick the generating model that looks best to you (and explain why) and set the length of each generated document to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_sport = tokenizer.encode('Woods cruises to eight shot', return_tensors='pt')\n",
    "input_ids_business = tokenizer.encode('FedEx raises first quarter', return_tensors='pt')\n",
    "input_ids_tech = tokenizer.encode('Space Station Crew Blast', return_tensors='pt')\n",
    "input_ids_world = tokenizer.encode('Plane Crashes in China', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85923f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output = model.generate(\n",
    "    input_ids_sport, \n",
    "    max_length=50, \n",
    "    num_beams=4, \n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    input_ids_sport, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.90, \n",
    "    top_k=0\n",
    ")\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f54a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    input_ids_business, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.90, \n",
    "    top_k=0\n",
    ")\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the beam approach generates more comprehensive text in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1303c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output = model.generate(\n",
    "    input_ids_business, \n",
    "    max_length=50, \n",
    "    num_beams=4, \n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    input_ids_world, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.90, \n",
    "    top_k=0\n",
    ")\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6716ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even though the top_p = 0.95, this seems quite ad-hoc\n",
    "# it seems that in general it doesn't perform that well on this snippet no matter the method chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    input_ids_tech, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.95, \n",
    "    top_k=0\n",
    ")\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    input_ids_tech, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.99, \n",
    "    top_k=0\n",
    ")\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output = model.generate(\n",
    "    input_ids_tech, \n",
    "    max_length=50, \n",
    "    num_beams=6, \n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    input_ids_tech, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c46f5ccc24ef453e97fd721763d326d36913f44a8394821f5f9ff49f0aa076c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
