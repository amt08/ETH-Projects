{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW03: Distance and Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Pre-process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "dfs = df.sample(200)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "##TODO pre-process text as you did in HW02\n",
    "dfs['sentences'] = dfs['text'].apply(lambda x: list(nlp(x).sents))\n",
    "dfs['tokens'] = dfs['text'].apply(lambda x: list(nlp(x)))\n",
    "\n",
    "def preprocess(text):\n",
    "    return ' '.join([word.lemma_.lower() for word in list(nlp(text)) if not word.is_punct and not word.is_stop])\n",
    "\n",
    "dfs['tokens_lower'] = dfs['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b3221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c209e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO vectorize the pre-processed text using CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=0.01,\n",
    "                             max_df=0.9,\n",
    "                             max_features = 1000,\n",
    "                             stop_words = 'english',\n",
    "                             ngram_range = (1, 2))\n",
    "\n",
    "X = vectorizer.fit_transform(dfs['tokens_lower'])\n",
    "\n",
    "cv_df = pd.DataFrame(X.toarray())\n",
    "cv_df.columns = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "##TODO compute the cosine similarity for the first 200 snippets and for the first snippet,\n",
    "# show the three most similar snippets and their respective cosine similarity scores\n",
    "similarity = cosine_similarity(cv_df.iloc[:200])\n",
    "first_snippet = similarity[0].copy() # cosine similarity for first snippet\n",
    "first_snippet[::-1].sort()\n",
    "\n",
    "print('First snippet:')\n",
    "print(dfs.iloc[0]['text'])\n",
    "print('\\n')\n",
    "\n",
    "for i in range(3):\n",
    "    print(dfs.iloc[np.where(similarity[0] == first_snippet[i+1])]['text'].values)\n",
    "    print(f'Cosine similarity is : {first_snippet[i+1]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3,svd_solver='randomized')\n",
    "\n",
    "##TODO reduce the vectorized data using PCA\n",
    "pca = PCA(n_components = 3, svd_solver = 'randomized')\n",
    "pca_df = pca.fit_transform(cv_df)\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "##TODO compute again cosine similarity with the reduced version for the first 200 snippets\n",
    "similarity_pca = cosine_similarity(pca_df[:200])\n",
    "\n",
    "##TODO for the first snippet, show again its three most similar snippets\n",
    "\n",
    "first_snipp = similarity_pca[0].copy() # cosine similarity for first snippet\n",
    "first_snipp[::-1].sort()\n",
    "\n",
    "print('First snippet:')\n",
    "print(dfs.iloc[0]['text'])\n",
    "print('\\n')\n",
    "\n",
    "for i in range(3):\n",
    "    print(dfs.iloc[np.where(similarity_pca[0] == first_snipp[i+1])]['text'].values)\n",
    "    print(f'Cosine similarity is : {first_snipp[i+1]}')\n",
    "    print('\\n')\n",
    "    \n",
    "# Yes, the results change masively. It seems like after PCA the texts are much closer \n",
    "# to the first snippet concerning the theme: Darfur / US. Interesting that Yahoo article gets such a high similarity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the cosine similarity before and after PCA reduction. Did the results change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "##TODO create the clusters found with k-means clustering nd 10 clusters\n",
    "no_clusters = 10\n",
    "k_means = KMeans(n_clusters=no_clusters, n_jobs = -1, random_state = 123)\n",
    "k_means.fit(X)\n",
    "clusters = k_means.labels_.tolist()\n",
    "dfs['cluster'] = clusters\n",
    "\n",
    "##TODO find the optimal number of clusters in a range from 2 to 50 using the silhouette score\n",
    "silhouette_score(X, k_means.labels_)\n",
    "s_score = list()\n",
    "\n",
    "for n in range(2, 50):\n",
    "    kmeans = KMeans(n_clusters = n, n_jobs = -1)\n",
    "    kmeans.fit(X)\n",
    "    s_score.append(silhouette_score(X, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7634de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(2, 50), s_score)\n",
    "plt.xlabel('No clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5575ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_sil_score = max(s_score[2:50])\n",
    "s_score.index(opt_sil_score)\n",
    "opt_num_cluster = range(2, 50)[s_score.index(opt_sil_score)]\n",
    "print('The optimal number of clusters is %s' %opt_num_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO create the clusters using the optimal number of clusters obtained before\n",
    "\n",
    "k_means = KMeans(n_clusters=opt_num_cluster, n_jobs = -1, random_state = 123)\n",
    "k_means.fit(X)\n",
    "clusters = k_means.labels_.tolist()\n",
    "dfs['cluster_optimal'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO compare the documents in cluster \"1\" under the two specifications, does the cluster look \n",
    "# cleaner after having searched for the optimal number of clusters?\n",
    "dfs[dfs['cluster_optimal'] == 1]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part you will need to use LDA Mallet. If you cannot have Mallet run, you can use the simple LDA algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim import corpora\n",
    "from random import shuffle\n",
    "\n",
    "##TODO create a dictionary with the pre-processed tokenized text\n",
    "# and filter it according to frequencies and keeping 1000 vocabularies\n",
    "\n",
    "\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "\n",
    "def preprocess(text, nlp):\n",
    "    return [word.lemma_.lower() for word in nlp(text)\n",
    "            if not word.is_punct and not word.is_stop and not word.is_digit \n",
    "            and len(word) > 2]\n",
    "\n",
    "documents = list()\n",
    "\n",
    "for doc in dfs['text']:\n",
    "    for paragraph in doc.split(\"\\n\\n\"):\n",
    "        documents.append(preprocess(paragraph, tokenizer))\n",
    "        \n",
    "shuffle(documents)\n",
    "\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "dictionary.filter_extremes(no_below = 5, no_above = 0.7, keep_n = 1000)\n",
    "\n",
    "##TODO create the doc_term_matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO train a LDA Mallet model with 5, 10 and 15 topics\n",
    "##TODO compute the coherence score for each of these model \n",
    "#and print the topics from the model with highest coherence score\n",
    "\n",
    "scores = list()\n",
    "# topics = list()\n",
    "\n",
    "for num_topics in [5, 10, 15]:\n",
    "\n",
    "    lda = LdaModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary, passes=2)\n",
    "    coherence = CoherenceModel(model = lda, texts = documents, corpus = doc_term_matrix,\n",
    "                              dictionary=dictionary, coherence='c_v')\n",
    "    scores.append((num_topics, coherence.get_coherence(), lda.show_topics(formatted=True), lda))\n",
    "    \n",
    "lda_models = pd.DataFrame(scores, columns=[\"Number of Topics\", \"Coherence Scores\", \"Topics\", 'LDA Model'])\n",
    "\n",
    "\n",
    "# topics from model with highest coherence score\n",
    "list(lda_models[lda_models['Coherence Scores'] == lda_models['Coherence Scores'].values.max()]['Topics'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "##TODO using LDAvis visualize the topics using the optimal number of topics\n",
    "\n",
    "best_model = lda_models[lda_models['Coherence Scores'] == lda_models['Coherence Scores'].values.max()]['LDA Model'].values[0]\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(best_model, doc_term_matrix, dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c46f5ccc24ef453e97fd721763d326d36913f44a8394821f5f9ff49f0aa076c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
